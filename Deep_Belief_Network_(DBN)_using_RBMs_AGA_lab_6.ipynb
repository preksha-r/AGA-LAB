{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Train a Deep Belief Network (DBN) using RBMs.\n",
        "Fine-tune with supervised learning."
      ],
      "metadata": {
        "id": "e9Lrrdwjijqs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6b2NN9IT8-w",
        "outputId": "c054eb25-0099-42b5-c265-b50a1ed99b7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "pip install numpy torch scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Activation, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "class RBM:\n",
        "    def __init__(self, visible_size, hidden_size, learning_rate=0.01, batch_size=100, n_epochs=10):\n",
        "        self.visible_size = visible_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.n_epochs = n_epochs\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.W = np.random.normal(0, 0.01, (visible_size, hidden_size))\n",
        "        self.visible_bias = np.zeros(visible_size)\n",
        "        self.hidden_bias = np.zeros(hidden_size)\n",
        "\n",
        "    def sample_hidden(self, visible_states):\n",
        "        \"\"\"Compute hidden probabilities and sample hidden states.\"\"\"\n",
        "        hidden_activations = np.dot(visible_states, self.W) + self.hidden_bias\n",
        "        hidden_probs = 1.0 / (1.0 + np.exp(-hidden_activations))\n",
        "        hidden_states = (hidden_probs > np.random.random(hidden_probs.shape)).astype(float)\n",
        "        return hidden_probs, hidden_states\n",
        "\n",
        "    def sample_visible(self, hidden_states):\n",
        "        \"\"\"Compute visible probabilities and sample visible states.\"\"\"\n",
        "        visible_activations = np.dot(hidden_states, self.W.T) + self.visible_bias\n",
        "        visible_probs = 1.0 / (1.0 + np.exp(-visible_activations))\n",
        "        visible_states = (visible_probs > np.random.random(visible_probs.shape)).astype(float)\n",
        "        return visible_probs, visible_states\n",
        "\n",
        "    def contrastive_divergence(self, visible_data):\n",
        "        \"\"\"Perform one step of contrastive divergence learning.\"\"\"\n",
        "        # Positive phase\n",
        "        pos_hidden_probs, pos_hidden_states = self.sample_hidden(visible_data)\n",
        "        pos_associations = np.dot(visible_data.T, pos_hidden_probs)\n",
        "\n",
        "        # Negative phase (reconstruction)\n",
        "        neg_visible_probs, neg_visible_states = self.sample_visible(pos_hidden_states)\n",
        "        neg_hidden_probs, neg_hidden_states = self.sample_hidden(neg_visible_probs)\n",
        "        neg_associations = np.dot(neg_visible_probs.T, neg_hidden_probs)\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.W += self.learning_rate * ((pos_associations - neg_associations) / len(visible_data))\n",
        "        self.visible_bias += self.learning_rate * np.mean(visible_data - neg_visible_probs, axis=0)\n",
        "        self.hidden_bias += self.learning_rate * np.mean(pos_hidden_probs - neg_hidden_probs, axis=0)\n",
        "\n",
        "        # Compute reconstruction error\n",
        "        error = np.mean((visible_data - neg_visible_probs) ** 2)\n",
        "        return error\n",
        "\n",
        "    def fit(self, data):\n",
        "        \"\"\"Train the RBM with the given data.\"\"\"\n",
        "        n_samples = data.shape[0]\n",
        "        n_batches = n_samples // self.batch_size\n",
        "\n",
        "        errors = []\n",
        "\n",
        "        for epoch in range(self.n_epochs):\n",
        "            epoch_error = 0\n",
        "            np.random.shuffle(data)  # Shuffle data for each epoch\n",
        "\n",
        "            for batch in range(n_batches):\n",
        "                batch_start = batch * self.batch_size\n",
        "                batch_end = (batch + 1) * self.batch_size\n",
        "                batch_data = data[batch_start:batch_end]\n",
        "\n",
        "                error = self.contrastive_divergence(batch_data)\n",
        "                epoch_error += error\n",
        "\n",
        "            avg_epoch_error = epoch_error / n_batches\n",
        "            errors.append(avg_epoch_error)\n",
        "            print(f\"Epoch {epoch+1}/{self.n_epochs}, Error: {avg_epoch_error:.4f}\")\n",
        "\n",
        "        return errors\n",
        "\n",
        "    def transform(self, data):\n",
        "        \"\"\"Transform data to hidden representation.\"\"\"\n",
        "        hidden_probs, _ = self.sample_hidden(data)\n",
        "        return hidden_probs\n",
        "\n",
        "\n",
        "class DBN:\n",
        "    def __init__(self, layer_sizes, learning_rate=0.01, batch_size=100, rbm_epochs=10):\n",
        "        \"\"\"\n",
        "        Initialize a Deep Belief Network\n",
        "\n",
        "        Args:\n",
        "            layer_sizes: list with the size of each layer (including input layer)\n",
        "            learning_rate: learning rate for RBMs\n",
        "            batch_size: batch size for RBMs\n",
        "            rbm_epochs: number of epochs for each RBM\n",
        "        \"\"\"\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.n_layers = len(layer_sizes) - 1  # Number of RBMs\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.rbm_epochs = rbm_epochs\n",
        "        self.rbm_layers = []\n",
        "\n",
        "        # Create RBM layers\n",
        "        for i in range(self.n_layers):\n",
        "            self.rbm_layers.append(\n",
        "                RBM(layer_sizes[i], layer_sizes[i+1],\n",
        "                    learning_rate=learning_rate,\n",
        "                    batch_size=batch_size,\n",
        "                    n_epochs=rbm_epochs)\n",
        "            )\n",
        "\n",
        "    def pretrain(self, data):\n",
        "        \"\"\"Perform greedy layer-wise pretraining.\"\"\"\n",
        "        print(\"Starting greedy layer-wise pretraining...\")\n",
        "        input_data = data\n",
        "\n",
        "        for i, rbm in enumerate(self.rbm_layers):\n",
        "            print(f\"Training RBM layer {i+1}/{self.n_layers}\")\n",
        "            rbm.fit(input_data)\n",
        "\n",
        "            # Transform data for the next RBM layer\n",
        "            input_data = rbm.transform(input_data)\n",
        "\n",
        "        print(\"Pretraining completed\")\n",
        "\n",
        "    def build_tensorflow_model(self, output_dim, activation='softmax'):\n",
        "        \"\"\"Build a TensorFlow model for fine-tuning.\"\"\"\n",
        "        model = Sequential()\n",
        "\n",
        "        # Add layers from pretrained RBMs\n",
        "        for i, rbm in enumerate(self.rbm_layers):\n",
        "            if i == 0:\n",
        "                model.add(Dense(rbm.hidden_size, input_dim=rbm.visible_size, activation='sigmoid'))\n",
        "            else:\n",
        "                model.add(Dense(rbm.hidden_size, activation='sigmoid'))\n",
        "\n",
        "            # Set weights from pretrained RBM\n",
        "            layer = model.layers[i]\n",
        "            layer.set_weights([rbm.W, rbm.hidden_bias])\n",
        "\n",
        "        # Add output layer\n",
        "        model.add(Dense(output_dim, activation=activation))\n",
        "\n",
        "        return model\n",
        "\n",
        "    def fine_tune(self, x_train, y_train, x_test, y_test, learning_rate=0.001, epochs=20, batch_size=128):\n",
        "        \"\"\"Fine-tune the network with supervised learning.\"\"\"\n",
        "        # One-hot encode the labels\n",
        "        output_dim = y_train.max() + 1\n",
        "        y_train_one_hot = to_categorical(y_train, output_dim)\n",
        "        y_test_one_hot = to_categorical(y_test, output_dim)\n",
        "\n",
        "        # Build and compile the model\n",
        "        model = self.build_tensorflow_model(output_dim)\n",
        "        optimizer = Adam(learning_rate=learning_rate)\n",
        "        model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "        print(\"Starting fine-tuning with supervised learning...\")\n",
        "        history = model.fit(\n",
        "            x_train, y_train_one_hot,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            validation_data=(x_test, y_test_one_hot),\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        print(\"Fine-tuning completed\")\n",
        "        return model, history\n",
        "\n",
        "\n",
        "# Example usage with MNIST dataset\n",
        "def run_mnist_example():\n",
        "    print(\"Loading MNIST dataset...\")\n",
        "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "    # Preprocess data\n",
        "    x_train = x_train.reshape(-1, 784).astype('float32') / 255.0\n",
        "    x_test = x_test.reshape(-1, 784).astype('float32') / 255.0\n",
        "\n",
        "    # Define DBN architecture\n",
        "    layer_sizes = [784, 500, 500, 200]\n",
        "\n",
        "    # Create and pretrain the DBN\n",
        "    dbn = DBN(\n",
        "        layer_sizes=layer_sizes,\n",
        "        learning_rate=0.01,\n",
        "        batch_size=100,\n",
        "        rbm_epochs=5\n",
        "    )\n",
        "\n",
        "    # Pretrain with unsupervised learning\n",
        "    dbn.pretrain(x_train)\n",
        "\n",
        "    # Fine-tune with supervised learning\n",
        "    model, history = dbn.fine_tune(\n",
        "        x_train, y_train,\n",
        "        x_test, y_test,\n",
        "        learning_rate=0.001,\n",
        "        epochs=10,\n",
        "        batch_size=128\n",
        "    )\n",
        "\n",
        "    # Evaluate the model\n",
        "    loss, accuracy = model.evaluate(x_test, to_categorical(y_test))\n",
        "    print(f\"Test accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    return dbn, model, history\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    dbn, model, history = run_mnist_example()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QR9HPhqqbBxt",
        "outputId": "686987d4-82ec-40dd-c791-96a472624865"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading MNIST dataset...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Starting greedy layer-wise pretraining...\n",
            "Training RBM layer 1/3\n",
            "Epoch 1/5, Error: 0.0563\n",
            "Epoch 2/5, Error: 0.0372\n",
            "Epoch 3/5, Error: 0.0305\n",
            "Epoch 4/5, Error: 0.0263\n",
            "Epoch 5/5, Error: 0.0236\n",
            "Training RBM layer 2/3\n",
            "Epoch 1/5, Error: 0.0522\n",
            "Epoch 2/5, Error: 0.0217\n",
            "Epoch 3/5, Error: 0.0137\n",
            "Epoch 4/5, Error: 0.0105\n",
            "Epoch 5/5, Error: 0.0094\n",
            "Training RBM layer 3/3\n",
            "Epoch 1/5, Error: 0.0533\n",
            "Epoch 2/5, Error: 0.0240\n",
            "Epoch 3/5, Error: 0.0148\n",
            "Epoch 4/5, Error: 0.0104\n",
            "Epoch 5/5, Error: 0.0080\n",
            "Pretraining completed\n",
            "Starting fine-tuning with supervised learning...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 21ms/step - accuracy: 0.1043 - loss: 2.3261 - val_accuracy: 0.1793 - val_loss: 2.2976\n",
            "Epoch 2/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.1057 - loss: 2.3033 - val_accuracy: 0.1382 - val_loss: 2.2981\n",
            "Epoch 3/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 24ms/step - accuracy: 0.1081 - loss: 2.3028 - val_accuracy: 0.1488 - val_loss: 2.2966\n",
            "Epoch 4/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 19ms/step - accuracy: 0.1075 - loss: 2.3027 - val_accuracy: 0.1717 - val_loss: 2.2960\n",
            "Epoch 5/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 20ms/step - accuracy: 0.1082 - loss: 2.3023 - val_accuracy: 0.1973 - val_loss: 2.2994\n",
            "Epoch 6/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.1094 - loss: 2.3017 - val_accuracy: 0.2021 - val_loss: 2.3010\n",
            "Epoch 7/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.1096 - loss: 2.3010 - val_accuracy: 0.1351 - val_loss: 2.3034\n",
            "Epoch 8/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - accuracy: 0.1088 - loss: 2.3006 - val_accuracy: 0.1618 - val_loss: 2.2996\n",
            "Epoch 9/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - accuracy: 0.1111 - loss: 2.2991 - val_accuracy: 0.1560 - val_loss: 2.3001\n",
            "Epoch 10/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 21ms/step - accuracy: 0.1137 - loss: 2.2970 - val_accuracy: 0.1450 - val_loss: 2.3054\n",
            "Fine-tuning completed\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.1505 - loss: 2.3038\n",
            "Test accuracy: 0.1450\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import BernoulliRBM\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Digits dataset (8x8 images of handwritten digits)\n",
        "digits = load_digits()\n",
        "X, y = digits.data, digits.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# --- Layer-wise RBM Pretraining ---\n",
        "rbm1 = BernoulliRBM(n_components=128, learning_rate=0.01, n_iter=20, verbose=1, random_state=42)\n",
        "X_train_rbm1 = rbm1.fit_transform(X_train_scaled)\n",
        "\n",
        "rbm2 = BernoulliRBM(n_components=64, learning_rate=0.01, n_iter=20, verbose=1, random_state=42)\n",
        "X_train_rbm2 = rbm2.fit_transform(X_train_rbm1)\n",
        "\n",
        "# --- Supervised Fine-Tuning with Logistic Regression ---\n",
        "clf = LogisticRegression(max_iter=1000, solver='lbfgs', multi_class='multinomial', random_state=42)\n",
        "clf.fit(X_train_rbm2, y_train)\n",
        "\n",
        "# Transform test data through RBMs\n",
        "X_test_rbm1 = rbm1.transform(X_test_scaled)\n",
        "X_test_rbm2 = rbm2.transform(X_test_rbm1)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = clf.predict(X_test_rbm2)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Stacked DBN (RBMs + LR) accuracy on Digits dataset: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PR0Wvv3AfYed",
        "outputId": "3efb28d1-1e6e-4f21-85de-1ff08a6ebfb0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BernoulliRBM] Iteration 1, pseudo-likelihood = -109.32, time = 0.15s\n",
            "[BernoulliRBM] Iteration 2, pseudo-likelihood = -124.54, time = 0.32s\n",
            "[BernoulliRBM] Iteration 3, pseudo-likelihood = -135.10, time = 0.27s\n",
            "[BernoulliRBM] Iteration 4, pseudo-likelihood = -174.39, time = 0.25s\n",
            "[BernoulliRBM] Iteration 5, pseudo-likelihood = -219.01, time = 0.16s\n",
            "[BernoulliRBM] Iteration 6, pseudo-likelihood = -262.25, time = 0.20s\n",
            "[BernoulliRBM] Iteration 7, pseudo-likelihood = -310.08, time = 0.19s\n",
            "[BernoulliRBM] Iteration 8, pseudo-likelihood = -359.88, time = 0.19s\n",
            "[BernoulliRBM] Iteration 9, pseudo-likelihood = -412.63, time = 0.14s\n",
            "[BernoulliRBM] Iteration 10, pseudo-likelihood = -463.32, time = 0.18s\n",
            "[BernoulliRBM] Iteration 11, pseudo-likelihood = -525.85, time = 0.24s\n",
            "[BernoulliRBM] Iteration 12, pseudo-likelihood = -579.42, time = 0.19s\n",
            "[BernoulliRBM] Iteration 13, pseudo-likelihood = -632.30, time = 0.25s\n",
            "[BernoulliRBM] Iteration 14, pseudo-likelihood = -681.14, time = 0.08s\n",
            "[BernoulliRBM] Iteration 15, pseudo-likelihood = -739.87, time = 0.09s\n",
            "[BernoulliRBM] Iteration 16, pseudo-likelihood = -795.09, time = 0.16s\n",
            "[BernoulliRBM] Iteration 17, pseudo-likelihood = -855.58, time = 0.10s\n",
            "[BernoulliRBM] Iteration 18, pseudo-likelihood = -915.55, time = 0.17s\n",
            "[BernoulliRBM] Iteration 19, pseudo-likelihood = -961.38, time = 0.25s\n",
            "[BernoulliRBM] Iteration 20, pseudo-likelihood = -1015.63, time = 0.19s\n",
            "[BernoulliRBM] Iteration 1, pseudo-likelihood = -56.66, time = 0.10s\n",
            "[BernoulliRBM] Iteration 2, pseudo-likelihood = -51.45, time = 0.08s\n",
            "[BernoulliRBM] Iteration 3, pseudo-likelihood = -44.65, time = 0.10s\n",
            "[BernoulliRBM] Iteration 4, pseudo-likelihood = -39.34, time = 0.18s\n",
            "[BernoulliRBM] Iteration 5, pseudo-likelihood = -36.58, time = 0.07s\n",
            "[BernoulliRBM] Iteration 6, pseudo-likelihood = -36.03, time = 0.16s\n",
            "[BernoulliRBM] Iteration 7, pseudo-likelihood = -33.67, time = 0.16s\n",
            "[BernoulliRBM] Iteration 8, pseudo-likelihood = -32.97, time = 0.07s\n",
            "[BernoulliRBM] Iteration 9, pseudo-likelihood = -33.77, time = 0.11s\n",
            "[BernoulliRBM] Iteration 10, pseudo-likelihood = -32.30, time = 0.14s\n",
            "[BernoulliRBM] Iteration 11, pseudo-likelihood = -31.38, time = 0.25s\n",
            "[BernoulliRBM] Iteration 12, pseudo-likelihood = -30.74, time = 0.10s\n",
            "[BernoulliRBM] Iteration 13, pseudo-likelihood = -31.38, time = 0.19s\n",
            "[BernoulliRBM] Iteration 14, pseudo-likelihood = -28.95, time = 0.16s\n",
            "[BernoulliRBM] Iteration 15, pseudo-likelihood = -28.08, time = 0.12s\n",
            "[BernoulliRBM] Iteration 16, pseudo-likelihood = -27.54, time = 0.13s\n",
            "[BernoulliRBM] Iteration 17, pseudo-likelihood = -26.98, time = 0.09s\n",
            "[BernoulliRBM] Iteration 18, pseudo-likelihood = -26.66, time = 0.08s\n",
            "[BernoulliRBM] Iteration 19, pseudo-likelihood = -26.78, time = 0.07s\n",
            "[BernoulliRBM] Iteration 20, pseudo-likelihood = -25.48, time = 0.07s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacked DBN (RBMs + LR) accuracy on Digits dataset: 0.7083\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E1Q7bGgqiEtd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}